Our infrastrucure is not big enough, as we cannot process all data in the same place, we do a lot of data movement.
We group data by machine and day.

raw data has the following layout:
e.g 

abelhinha
  |-- 20111021102209-abelhinha.log.0
  |-- ...
  |-- 20111021102209-abelhinha.log.97
  |-- 20111021102209-abelhinha.log.98

file name follows this pattern %Y%m%d%H%M%-machine.log.$seq

steps to process data
 1. we filter raw data and store output grouping by machine and day
    e.g
    abelhinha
      |-- 20111021
            |-- 20111021102209-abelhinha.log.0.filtered
            |--  ...
            |-- 20111021102209-abelhinha.log.97.filtered
            |-- 20111021102209-abelhinha.log.98.filtered

   Usage: bash filter_and_group.sh $raw_data_dir $output_data_dir

2. sorts *filtered files. After sorting delete filtered files. It outputs *filtered.sort files
   Usage: bash sort_and_remove_old_data.sh $filtered_data_dir

3. Concats *filtered.sort files into based on creation date and sequence number.
   It sorts this concatened file
   Usage: python join_by_date_and_sort.py $data_dir
