Experiment uses input data and code stored and LSD nfs. To do so, we need to 
mount this file system at worker nodes.

We also need to sync worker node clocks, jobs should start at the same time.

So, before proper execution we need the following config steps
    - alloc. instances. see "./allocate_cloudgley.sh $the_number $emi"
    - config no-pass. see "./config_cloudgley_ssh.sh"
    - config vm naming (to avoid config dns). see config_hostname.sh config_hostname.py
    - config vm image. see "./config_apt.sh"
    - install packages  see "install_libs.sh"
    - sync clocks. see "./sync_clock.sh"
    - stage-in scripts see "./stage_in_replay_input.sh"
    - stage-in input see "./stage_in_replay_scripts.sh"
    - generate beefs install (update configs for each beefs component)
    - mount vm raw_dir filesystem see "./config_raw_dir.sh"
    - create outdirs see "./create_vm_out_dir.sh"

Each sample runs according to this script:

    for i in range(num_samples)
        #cleaning system
	foreach worker_node:
	    umount(replay_file_system)
            stop(honeybee(worker_node)
            data_server = ds(worker_node)
            stop(data_server)
            delete_metadata(data_server)
            rollback_raw_data(data_server)
            rollback_metadata(data_server)

        stop(queenbee)
        delete_metadata(queenbee)
        rollback_metadata(queenbee)

	#starting system up
        start(queenbee)
	foreach worker_node:
            data_server = ds(worker_node)
	    start(data_server)
            start(honeybee(worker_node))

	foreach worker_node:
	    #they should start at the same time
	    wait_and_start(worker_node, start_stamp)

	wait_replay(worker_nodes)
	foreach worker_node:
	    copy_results(worker_node)#copy to nfs home
